{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a59d71ce3a0d>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-2-a59d71ce3a0d>:93: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "iter 0 testing Accuracy 0.0858 Training Accuaracy 0.086\n",
      "iter 10 testing Accuracy 0.1192 Training Accuaracy 0.1209\n",
      "iter 20 testing Accuracy 0.155 Training Accuaracy 0.1566\n",
      "iter 30 testing Accuracy 0.1966 Training Accuaracy 0.1989\n",
      "iter 40 testing Accuracy 0.238 Training Accuaracy 0.237\n",
      "iter 50 testing Accuracy 0.2763 Training Accuaracy 0.2715\n",
      "iter 60 testing Accuracy 0.3168 Training Accuaracy 0.3114\n",
      "iter 70 testing Accuracy 0.3622 Training Accuaracy 0.3549\n",
      "iter 80 testing Accuracy 0.3751 Training Accuaracy 0.3705\n",
      "iter 90 testing Accuracy 0.3826 Training Accuaracy 0.3802\n",
      "iter 100 testing Accuracy 0.3984 Training Accuaracy 0.3975\n",
      "iter 110 testing Accuracy 0.4207 Training Accuaracy 0.4195\n",
      "iter 120 testing Accuracy 0.4295 Training Accuaracy 0.4305\n",
      "iter 130 testing Accuracy 0.4394 Training Accuaracy 0.4441\n",
      "iter 140 testing Accuracy 0.4297 Training Accuaracy 0.4396\n",
      "iter 150 testing Accuracy 0.4331 Training Accuaracy 0.4433\n",
      "iter 160 testing Accuracy 0.5113 Training Accuaracy 0.5121\n",
      "iter 170 testing Accuracy 0.5421 Training Accuaracy 0.5403\n",
      "iter 180 testing Accuracy 0.5494 Training Accuaracy 0.5468\n",
      "iter 190 testing Accuracy 0.557 Training Accuaracy 0.5517\n",
      "iter 200 testing Accuracy 0.5671 Training Accuaracy 0.5612\n",
      "iter 210 testing Accuracy 0.5844 Training Accuaracy 0.5792\n",
      "iter 220 testing Accuracy 0.6048 Training Accuaracy 0.5963\n",
      "iter 230 testing Accuracy 0.6108 Training Accuaracy 0.5988\n",
      "iter 240 testing Accuracy 0.6232 Training Accuaracy 0.6126\n",
      "iter 250 testing Accuracy 0.6479 Training Accuaracy 0.6415\n",
      "iter 260 testing Accuracy 0.6614 Training Accuaracy 0.6535\n",
      "iter 270 testing Accuracy 0.6623 Training Accuaracy 0.6542\n",
      "iter 280 testing Accuracy 0.6686 Training Accuaracy 0.6619\n",
      "iter 290 testing Accuracy 0.6693 Training Accuaracy 0.6617\n",
      "iter 300 testing Accuracy 0.6727 Training Accuaracy 0.6672\n",
      "iter 310 testing Accuracy 0.6778 Training Accuaracy 0.671\n",
      "iter 320 testing Accuracy 0.6802 Training Accuaracy 0.6734\n",
      "iter 330 testing Accuracy 0.6821 Training Accuaracy 0.6773\n",
      "iter 340 testing Accuracy 0.6894 Training Accuaracy 0.6833\n",
      "iter 350 testing Accuracy 0.6961 Training Accuaracy 0.6917\n",
      "iter 360 testing Accuracy 0.7086 Training Accuaracy 0.7028\n",
      "iter 370 testing Accuracy 0.7153 Training Accuaracy 0.7107\n",
      "iter 380 testing Accuracy 0.7155 Training Accuaracy 0.7137\n",
      "iter 390 testing Accuracy 0.7276 Training Accuaracy 0.7248\n",
      "iter 400 testing Accuracy 0.7286 Training Accuaracy 0.726\n",
      "iter 410 testing Accuracy 0.7317 Training Accuaracy 0.7283\n",
      "iter 420 testing Accuracy 0.7524 Training Accuaracy 0.7454\n",
      "iter 430 testing Accuracy 0.7781 Training Accuaracy 0.7652\n",
      "iter 440 testing Accuracy 0.7903 Training Accuaracy 0.7759\n",
      "iter 450 testing Accuracy 0.7907 Training Accuaracy 0.7743\n",
      "iter 460 testing Accuracy 0.7859 Training Accuaracy 0.7725\n",
      "iter 470 testing Accuracy 0.79 Training Accuaracy 0.7771\n",
      "iter 480 testing Accuracy 0.8109 Training Accuaracy 0.7993\n",
      "iter 490 testing Accuracy 0.8268 Training Accuaracy 0.8152\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "\n",
    "#每个批次的大小\n",
    "batch_size=100\n",
    "#计算一共有多少个批次\n",
    "a_batch=mnist.train.num_examples//batch_size\n",
    "\n",
    "#初始化权重\n",
    "def weight_variable(shape,name):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "#初始化偏置\n",
    "def bias_variable(shape,name):\n",
    "    initial=tf.constant(0,1,shape=shape)\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],\n",
    "                          strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "#命名空间\n",
    "with tf.name_scope(\"input\"):\n",
    "    #定义两个placeholder\n",
    "    x=tf.placeholder(tf.float32,[None,784],name=\"x_input\")\n",
    "    y=tf.placeholder(tf.float32,[None,10],name=\"y_input\")\n",
    "    with tf.name_scope(\"x_image\"):\n",
    "        x_image=tf.reshape(x,[-1,28,28,1],name=\"x_image\")\n",
    "\n",
    "with tf.name_scope(\"Conv1\"):\n",
    "    #初始化第一个卷积层的权重和偏置\n",
    "    with tf.name_scope(\"W_conv1\"):\n",
    "        W_conv1=weight_variable([5,5,1,10],name=\"W_conv1\")\n",
    "        #5*5的采样窗口，6个卷积核从一个平面抽取特征\n",
    "    with tf.name_scope(\"b_conv1\"):\n",
    "        b_conv1=bias_variable([10],name=\"b_conv1\")\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "    with tf.name_scope(\"h_pool1\"):\n",
    "        h_pool1=max_pool_2x2(h_conv1)\n",
    "        #池化\n",
    "\n",
    "with tf.name_scope(\"Conv2\"):\n",
    "    with tf.name_scope(\"W_conv2\"):\n",
    "        W_conv2=weight_variable([5,5,10,10],name=\"W_conv2\")\n",
    "    with tf.name_scope(\"b_conv2\"):\n",
    "        b_conv2=bias_variable([10],name=\"b_conv2\")\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "    with tf.name_scope(\"h_pool2\"):\n",
    "        h_pool2=max_pool_2x2(h_conv2)\n",
    "\n",
    "#28*28的图片第一次卷积后是28*28，第一次池化后是14*14\n",
    "#第二次卷积后是14*14，第二次池化后是7*7\n",
    "\n",
    "with tf.name_scope(\"fe1\"):\n",
    "    #定义第一个全连接层的权重\n",
    "    with tf.name_scope(\"W_fe1\"):\n",
    "        W_fe1=weight_variable([7*7*10,60],name=\"W_fe1\")\n",
    "    with tf.name_scope(\"b_fe1\"):\n",
    "        b_fe1=bias_variable([60],name=\"b_fe1\")\n",
    "    \n",
    "    with tf.name_scope(\"h_pool2_flat\"):\n",
    "        #把池化层2的输出扁平化\n",
    "        h_pool2_flat=tf.reshape(h_pool2,[-1,7*7*10],name=\"h_pool2_flat\")\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        h_fel=tf.nn.relu(tf.matmul(h_pool2_flat,W_fe1)+b_fe1)\n",
    "   \n",
    "    with tf.name_scope(\"keep_prob\"):\n",
    "        keep_prob=tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    with tf.name_scope(\"h_fe1_drop\"):\n",
    "        h_fe1_drop=tf.nn.dropout(h_fel,keep_prob,name=\"h_fe1_drop\")\n",
    "\n",
    "with tf.name_scope(\"fe2\"):\n",
    "    #定义第二个全连接层\n",
    "    with tf.name_scope(\"W_fe2\"):\n",
    "        W_fe2=weight_variable([60,10],name=\"W_fe2\")\n",
    "    with tf.name_scope(\"b_fe2\"):\n",
    "        b_fe2=bias_variable([10],name=\"b_fe2\")\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        #计算输出\n",
    "        prediction=tf.nn.softmax(tf.matmul(h_fe1_drop,W_fe2)+b_fe2)\n",
    "\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    #交叉熵代价函数\n",
    "    cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                             (labels=y,logits=prediction),name=\"cross_entropy\")\n",
    "    tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #优化\n",
    "    train_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    with tf.name_scope(\"correct_prediction\"):\n",
    "        #结果存放在一个布尔型列表中\n",
    "        correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        #求准确率\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "#合并所有的summary\n",
    "merged=tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer=tf.summary.FileWriter(\"logs/train\",sess.graph)\n",
    "    test_writer=tf.summary.FileWriter(\"logs/test\",sess.graph)\n",
    "    \n",
    "    for i in range(500):\n",
    "        #训练模型\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,\n",
    "                                           keep_prob:0.5})\n",
    "        #记录训练集计算的参数\n",
    "        summary=sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1})\n",
    "        train_writer.add_summary(summary,i)\n",
    "        \n",
    "        #记录测试集计算的参数\n",
    "        batch_xs,batch_ys=mnist.test.next_batch(batch_size)\n",
    "        summary=sess.run(merged,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1})\n",
    "        test_writer.add_summary(summary,i)\n",
    "        \n",
    "        if i%10==0:\n",
    "            test_acc=sess.run(accuracy,feed_dict={x:mnist.test.images,\n",
    "                                         y:mnist.test.labels\n",
    "                                         ,keep_prob:1.0})\n",
    "            train_acc=sess.run(accuracy,feed_dict={x:mnist.train.images[:10000],\n",
    "                                         y:mnist.train.labels[:10000]\n",
    "                                         ,keep_prob:1.0}) \n",
    "            print('iter',str(i),'testing Accuracy',str(test_acc),\n",
    "                  \"Training Accuaracy\",str(train_acc))\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
